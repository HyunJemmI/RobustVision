{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b46406-ee9f-4dce-ac32-334f5988b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 총 3626개의 20.jpg 파일을 '/home1/hyunje0/contrastive_test_set'에 복사했습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 입력 루트 (TuSimple 원본)\n",
    "input_root = \"/home1/hyunje0/.cache/kagglehub/datasets/manideep1108/tusimple/versions/5/TUSimple/train_set/clips\"\n",
    "\n",
    "# 출력 루트 (Test Set 대용)\n",
    "output_root = \"/home1/hyunje0/contrastive_test_set\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "# 사용 클립 리스트\n",
    "clip_dirs = [\"0313-1\", \"0313-2\", \"0531\", \"0601\"]\n",
    "\n",
    "# counter = 0\n",
    "# for clip in clip_dirs:\n",
    "#     clip_path = os.path.join(input_root, clip)\n",
    "#     for timestamp in os.listdir(clip_path):\n",
    "#         timestamp_path = os.path.join(clip_path, timestamp)\n",
    "#         target_image = os.path.join(timestamp_path, \"20.jpg\")\n",
    "\n",
    "#         if os.path.isfile(target_image):\n",
    "#             # 새로운 파일명: clip_timestamp.jpg\n",
    "#             new_filename = f\"{clip}_{timestamp}.jpg\"\n",
    "#             shutil.copy(target_image, os.path.join(output_root, new_filename))\n",
    "#             counter += 1\n",
    "\n",
    "counter = 0\n",
    "for clip in clip_dirs:\n",
    "    clip_path = os.path.join(input_root, clip)\n",
    "    for timestamp in os.listdir(clip_path):\n",
    "        timestamp_path = os.path.join(clip_path, timestamp)\n",
    "        target_image = os.path.join(timestamp_path, \"20.jpg\")\n",
    "\n",
    "        if os.path.isfile(target_image):\n",
    "            counter += 1\n",
    "        \n",
    "\n",
    "print(f\"✅ 총 {counter}개의 20.jpg 파일을 '{output_root}'에 복사했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceea3144-8d0d-4c9c-a010-30fbfe9d0a2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /myubai/lib/python3.12/site-packages (24.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.3.1\n",
      "    Uninstalling pip-24.3.1:\n",
      "      Successfully uninstalled pip-24.3.1\n",
      "Successfully installed pip-25.1.1\n",
      "Requirement already satisfied: tqdm in /myubai/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: pillow in /myubai/lib/python3.12/site-packages (11.1.0)\n",
      "Requirement already satisfied: numpy in /myubai/lib/python3.12/site-packages (2.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install tqdm pillow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa297b8-51b7-44d5-8c29-560c1099ca3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "# ─── 1. SimCLRModel 정의 (학습 모델과 동일하게) ─────────────────\n",
    "class SimCLRModel(nn.Module):\n",
    "    def __init__(self, backbone='resnet18', projection_dim=256):\n",
    "        super().__init__()\n",
    "        base = torchvision.models.resnet50(weights=None) if backbone == 'resnet18' else torchvision.models.resnet18(weights=None)\n",
    "        self.encoder = nn.Sequential(*list(base.children())[:-1])  # remove fc\n",
    "        feat_dim = base.fc.in_features\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)            # shape: [B, feat_dim, 1, 1]\n",
    "        h = h.view(h.size(0), -1)      # flatten: [B, feat_dim]\n",
    "        z = self.projector(h)          # [B, projection_dim]\n",
    "        return F.normalize(z, dim=1)   # L2 normalize across feature dim\n",
    "\n",
    "\n",
    "# ─── 2. 노이즈 추가 함수 ─────────────────────────────\n",
    "def add_light_noise_with_level(img: Image.Image, level: int) -> Image.Image:\n",
    "    if level == 0:\n",
    "        return img.copy()\n",
    "\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "    radius = 5 * level\n",
    "    intensity = 0.15 * level\n",
    "    blur = img.filter(ImageFilter.GaussianBlur(radius))\n",
    "    arr = np.clip(arr + intensity * np.array(blur), 0, 255)\n",
    "\n",
    "    h, w, _ = arr.shape\n",
    "    cx = int(w * random.uniform(0.3, 0.7))\n",
    "    cy = int(h * random.uniform(0.0, 0.3))\n",
    "    r = int(min(h, w) * 0.1 * level)\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    mask = np.exp(-((x - cx) ** 2 + (y - cy) ** 2) / (2 * (r / 2) ** 2))\n",
    "    arr = np.clip(arr + (mask[..., None] * (0.1 * level)), 0, 255)\n",
    "\n",
    "    gamma = 1.0 + 0.1 * level\n",
    "    inv = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** inv) * 255 for i in range(256)]).astype(np.uint8)\n",
    "    arr = table[arr.astype(np.uint8)]\n",
    "\n",
    "    return Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "\n",
    "# ─── 3. transform 및 임베딩 함수 ─────────────────────\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "def get_embedding(model, img: Image.Image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tensor = transform(img).unsqueeze(0).cuda()   # [1, 3, 224, 224]\n",
    "        feat = model(tensor)                          # [1, 256]\n",
    "        return feat.squeeze(0)                        # → [256]\n",
    "\n",
    "\n",
    "# ─── 4. Cosine Similarity 평가 함수 ────────────────────────────────\n",
    "def evaluate_similarity(model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 모델 로드\n",
    "    model = SimCLRModel(backbone='resnet18', projection_dim=256).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # 테스트 이미지 5장\n",
    "    test_root = \"/home1/hyunje0/contrastive_test_set\"\n",
    "    image_list = sorted([f for f in os.listdir(test_root) if f.endswith(\".jpg\")])[:5]\n",
    "\n",
    "    # 비교할 레벨 쌍\n",
    "    compare_pairs = [(0, 1), (0, 3), (0, 5), (0, 8), (3, 7)]\n",
    "\n",
    "    print(\"Cosine Similarity between noise levels using SimCLR model:\\n\")\n",
    "\n",
    "    for fn in tqdm(image_list):\n",
    "        img = Image.open(os.path.join(test_root, fn)).convert(\"RGB\")\n",
    "        level_feats = {}\n",
    "\n",
    "        for lvl in range(9):  # 0~8\n",
    "            noisy = add_light_noise_with_level(img, level=lvl)\n",
    "            feat = get_embedding(model, noisy)\n",
    "            level_feats[lvl] = feat\n",
    "\n",
    "        print(f\"[{fn}]\")\n",
    "        for a, b in compare_pairs:\n",
    "            sim = F.cosine_similarity(level_feats[a], level_feats[b], dim=0).item()\n",
    "            print(f\"  - level {a}-{b}: cosine similarity = {sim:.4f}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# ─── 5. 실행 ───────────────────────────────\n",
    "evaluate_similarity(\"Models/best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebc74982-37e3-47e8-be73-f9cdaeba62e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m     17\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mshow_selected_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mshow_selected_images\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, fname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_list):\n\u001b[0;32m----> 9\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_root, fname))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(img)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_selected_images():\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    test_root = \"/home1/hyunje0/contrastive_test_set\"\n",
    "    image_list = sorted([f for f in os.listdir(test_root) if f.endswith(\".jpg\")])[:5]\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, fname in enumerate(image_list):\n",
    "        img = Image.open(os.path.join(test_root, fname)).convert(\"RGB\")\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(img)\n",
    "    \n",
    "        plt.title(fname, fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(\"Selected TuSimple Test Images (Top 5)\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_selected_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d181d-578a-47ac-a4f3-7c8c202bf40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, random\n",
    "from PIL import Image, ImageFilter\n",
    "def show_augmented_images():\n",
    "    test_root = \"/home1/hyunje0/contrastive_test_set\"\n",
    "    image_list = random.sample(\n",
    "    [f for f in os.listdir(test_root) if f.endswith(\".jpg\")],\n",
    "    k=5)\n",
    "\n",
    "    for fn in image_list:\n",
    "        img = Image.open(os.path.join(test_root, fn)).convert(\"RGB\")\n",
    "        plt.figure(figsize=(18, 2.5))\n",
    "        for lvl in range(9):  # level 0~8\n",
    "            aug = add_light_noise_with_level(img, level=lvl)\n",
    "            plt.subplot(1, 9, lvl+1)\n",
    "            plt.imshow(aug)\n",
    "            plt.title(f\"lv {lvl}\", fontsize=8)\n",
    "            plt.axis(\"off\")\n",
    "        plt.suptitle(f\"{fn} - Noise Levels 0~8\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 실행\n",
    "show_augmented_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4973f0-0089-4e4b-8bf8-e0b503161f62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ─── 환경 설정 ──────────────────────────────────────────────────────────────\n",
    "import os, random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 디바이스 설정 ──────────────────────────────────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# ─── 모델 정의 ──────────────────────────────────────────────────────────────\n",
    "class SimCLR_Eval(nn.Module):\n",
    "    def __init__(self, proj_dim=128):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet18(weights=None)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-2], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, proj_dim, bias=False), nn.BatchNorm1d(proj_dim, affine=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x).flatten(1)\n",
    "        z = F.normalize(self.projector(h), dim=1)\n",
    "        return z\n",
    "\n",
    "# ─── 모델 로드 ──────────────────────────────────────────────────────────────\n",
    "model = SimCLR_Eval(proj_dim=128).to(device)\n",
    "ckpt = torch.load(\"best_model.pth\", map_location=device)\n",
    "model.load_state_dict(ckpt, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# ─── 노이즈 추가 함수 ───────────────────────────────────────────────────────\n",
    "def add_light_noise_with_level(img: Image.Image, level: int) -> Image.Image:\n",
    "    if level == 0:\n",
    "        return img.copy()\n",
    "\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "    radius = 5 * level\n",
    "    intensity = 0.15 * level\n",
    "    blur = img.filter(ImageFilter.GaussianBlur(radius))\n",
    "    arr = np.clip(arr + intensity * np.array(blur), 0, 255)\n",
    "\n",
    "    h, w, _ = arr.shape\n",
    "    cx = int(w * random.uniform(0.3, 0.7))\n",
    "    cy = int(h * random.uniform(0.0, 0.3))\n",
    "    r = int(min(h, w) * 0.1 * level)\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    mask = np.exp(-((x - cx) ** 2 + (y - cy) ** 2) / (2 * (r / 2) ** 2))\n",
    "    arr = np.clip(arr + (mask[..., None] * (0.1 * level)), 0, 255)\n",
    "\n",
    "    gamma = 1.0 + 0.1 * level\n",
    "    inv = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** inv) * 255 for i in range(256)]).astype(np.uint8)\n",
    "    arr = table[arr.astype(np.uint8)]\n",
    "\n",
    "    return Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "# ─── 전처리 및 임베딩 추출 ───────────────────────────────────────────────────\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "])\n",
    "\n",
    "def get_embedding(img: Image.Image):\n",
    "    with torch.no_grad():\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        f = model(x).squeeze()\n",
    "        return f / f.norm()\n",
    "\n",
    "# ─── 테스트 이미지 유사도 계산 ───────────────────────────────────────────────\n",
    "base_path = \"/home1/hyunje0/contrastive_test_set\"\n",
    "levels = [0, 3, 5, 8]\n",
    "\n",
    "image_list = random.sample(\n",
    "    [f for f in os.listdir(base_path) if f.endswith(\".jpg\")],\n",
    "    k=5\n",
    ")\n",
    "\n",
    "for fname in tqdm(image_list, desc=\"Cosine Similarity on Test Images\"):\n",
    "    img = Image.open(os.path.join(base_path, fname)).convert(\"RGB\")\n",
    "    features = {}\n",
    "    visuals = {}\n",
    "\n",
    "    for lv in levels:\n",
    "        aug = add_light_noise_with_level(img, lv)\n",
    "        feat = get_embedding(aug)\n",
    "        features[lv] = feat\n",
    "        visuals[lv] = aug.copy()\n",
    "\n",
    "    print(f\"\\n📄 {fname}\")\n",
    "    for lv in levels[1:]:\n",
    "        sim = F.cosine_similarity(features[0], features[lv], dim=0).item()\n",
    "        print(f\"  - lv0 vs lv{lv}: cosine similarity = {sim:.4f}\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "    for i, lv in enumerate(levels):\n",
    "        axes[i].imshow(visuals[lv])\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"lv{lv}\")\n",
    "    plt.suptitle(f\"{fname} - Noise Levels 0/3/5/8\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d2021-bc32-4fb9-bf90-3a54c91fd597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 1. 모델 정의 및 로딩\n",
    "# ───────────────────────────────\n",
    "class SimCLR_Eval(nn.Module):\n",
    "    def __init__(self, proj_dim=128):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet18(weights=None)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-2], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, proj_dim, bias=False), nn.BatchNorm1d(proj_dim, affine=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x).flatten(1)\n",
    "        z = F.normalize(self.projector(h), dim=1)\n",
    "        return z\n",
    "\n",
    "# ✅ 모델 로드 (encoder + projector 모두 로딩)\n",
    "model = SimCLR_Eval(proj_dim=128)\n",
    "ckpt = torch.load(\"Models/best_model.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 2. 노이즈 함수 정의\n",
    "# ───────────────────────────────\n",
    "def add_light_noise_with_level(img: Image.Image, level: int) -> Image.Image:\n",
    "    if level == 0:\n",
    "        return img.copy()\n",
    "\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "    radius = 5 * level\n",
    "    intensity = 0.15 * level\n",
    "    blur = img.filter(ImageFilter.GaussianBlur(radius))\n",
    "    arr = np.clip(arr + intensity * np.array(blur), 0, 255)\n",
    "\n",
    "    h, w, _ = arr.shape\n",
    "    cx = int(w * random.uniform(0.3, 0.7))\n",
    "    cy = int(h * random.uniform(0.0, 0.3))\n",
    "    r = int(min(h, w) * 0.1 * level)\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    mask = np.exp(-((x - cx) ** 2 + (y - cy) ** 2) / (2 * (r / 2) ** 2))\n",
    "    arr = np.clip(arr + (mask[..., None] * (0.1 * level)), 0, 255)\n",
    "\n",
    "    gamma = 1.0 + 0.1 * level\n",
    "    inv = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** inv) * 255 for i in range(256)]).astype(np.uint8)\n",
    "    arr = table[arr.astype(np.uint8)]\n",
    "\n",
    "    return Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 3. 전처리 및 임베딩 추출 함수\n",
    "# ───────────────────────────────\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "])\n",
    "\n",
    "def get_embedding(img: Image.Image):\n",
    "    with torch.no_grad():\n",
    "        x = transform(img).unsqueeze(0)\n",
    "        f = model(x).squeeze()\n",
    "        return f / f.norm()\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 4. 이미지 로딩 및 cosine similarity 계산\n",
    "# ───────────────────────────────\n",
    "base_path = \"/home1/hyunje0/contrastive_test_set\"\n",
    "levels = [0, 3, 5, 8]\n",
    "\n",
    "image_list = random.sample(\n",
    "    [f for f in os.listdir(base_path) if f.endswith(\".jpg\")],\n",
    "    k=5\n",
    ")\n",
    "\n",
    "for fname in tqdm(image_list, desc=\"Cosine Similarity on Test Images\"):\n",
    "    img = Image.open(os.path.join(base_path, fname)).convert(\"RGB\")\n",
    "    features = {}\n",
    "    visuals = {}\n",
    "\n",
    "    for lv in levels:\n",
    "        aug = add_light_noise_with_level(img, lv)\n",
    "        feat = get_embedding(aug)\n",
    "        features[lv] = feat\n",
    "        visuals[lv] = aug.copy()\n",
    "\n",
    "    # Cosine Similarity 출력\n",
    "    print(f\"\\n📄 {fname}\")\n",
    "    for lv in levels[1:]:\n",
    "        sim = F.cosine_similarity(features[0], features[lv], dim=0).item()\n",
    "        print(f\"  - lv0 vs lv{lv}: cosine similarity = {sim:.4f}\")\n",
    "\n",
    "    # 시각화\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "    for i, lv in enumerate(levels):\n",
    "        axes[i].imshow(visuals[lv])\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"lv{lv}\")\n",
    "    plt.suptitle(f\"{fname} - Noise Levels 0/3/5/8\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16973ed-5b23-4e5a-98ee-b2a0b3147523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 0. Config & Device\n",
    "# ───────────────────────────────\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# data_dir = \"/home1/hyunje0/.cache/kagglehub/datasets/solesensei/solesensei_bdd100k/versions/2/aug_8levels/img\"\n",
    "data_dir = \"/home1/hyunje0/contrastive_test_set\"\n",
    "model_path = \"Models/best_model.pth\"\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 1. 모델 정의 (encoder + projector)\n",
    "# ───────────────────────────────\n",
    "def resnet18_backbone():\n",
    "    m = torchvision.models.resnet18(weights=None)\n",
    "    return nn.Sequential(*list(m.children())[:-2], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = resnet18_backbone()\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, proj_dim, bias=False), nn.BatchNorm1d(proj_dim, affine=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x).flatten(1)\n",
    "        z = F.normalize(self.projector(h), dim=1)\n",
    "        return z\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 2. 모델 로딩\n",
    "# ───────────────────────────────\n",
    "model = SimCLR(proj_dim=128).to(device)\n",
    "ckpt = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(ckpt, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 3. Transform + Embedding 함수\n",
    "# ───────────────────────────────\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "])\n",
    "\n",
    "def get_embedding(img: Image.Image):\n",
    "    with torch.no_grad():\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        z = model(x).squeeze()\n",
    "        return z / z.norm()\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 4. Collapse 여부 검증\n",
    "# ───────────────────────────────\n",
    "image_list = [f for f in os.listdir(data_dir) if f.endswith((\".jpg\", \".png\"))]\n",
    "sampled = random.sample(image_list, min(100, len(image_list)))  # 최대 100장\n",
    "\n",
    "feats = []\n",
    "for fname in sampled:\n",
    "    img = Image.open(os.path.join(data_dir, fname)).convert(\"RGB\")\n",
    "    feats.append(get_embedding(img))\n",
    "\n",
    "all_feats = torch.stack(feats)  # [N, D]\n",
    "mean_std = all_feats.std(dim=0).mean().item()\n",
    "\n",
    "print(f\"\\n📊 평균 Feature 표준편차 (차원별 std의 평균): {mean_std:.6f}\")\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 5. 해석\n",
    "# ───────────────────────────────\n",
    "if mean_std < 0.01:\n",
    "    print(\"⚠️ Feature collapse 의심: 대부분 임베딩이 동일합니다.\")\n",
    "elif mean_std < 0.05:\n",
    "    print(\"⚠️ Feature 다양성이 부족할 수 있음 (약한 collapse 가능)\")\n",
    "else:\n",
    "    print(\"✅ Feature 다양성이 충분합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5aed50-06bc-4dbd-a215-1ef66f087a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"✅ CUDA 사용 가능 (GPU 사용 중) — GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ CUDA 사용 불가 (CPU 사용 중)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7028a-8c65-4b3a-9806-e9340d68019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------구분선---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51955842-5aab-477a-a3c1-d9bd0f0bc4a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_imgs \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(base_path) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      2\u001b[0m base_imgs \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(all_imgs, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "all_imgs = [f for f in os.listdir(base_path) if f.endswith(\".jpg\")]\n",
    "base_imgs = random.sample(all_imgs, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7712997-d997-48c9-8b4e-845db6b86c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 1. 데이터 준비\n",
    "all_imgs = [f for f in os.listdir(base_path) if f.endswith(\".jpg\")]\n",
    "base_imgs = random.sample(all_imgs, k=100)\n",
    "\n",
    "same_sims = defaultdict(list)\n",
    "diff_sims = []\n",
    "\n",
    "for fname in tqdm(base_imgs, desc=\"Experiment loops\"):\n",
    "    img = Image.open(os.path.join(base_path, fname)).convert(\"RGB\")\n",
    "    emb0 = get_embedding(img)\n",
    "\n",
    "    # 2. 동일 이미지 vs 노이즈\n",
    "    for lv in [3,5,8]:\n",
    "        aug = add_light_noise_with_level(img, lv)\n",
    "        embn = get_embedding(aug)\n",
    "        sim = F.cosine_similarity(emb0, embn, dim=0).item()\n",
    "        same_sims[lv].append(sim)\n",
    "\n",
    "    # 3. 서로 다른 이미지 간 유사도\n",
    "    other = random.choice([x for x in all_imgs if x != fname])\n",
    "    img2 = Image.open(os.path.join(base_path, other)).convert(\"RGB\")\n",
    "    emb2 = get_embedding(img2)\n",
    "    sim_diff = F.cosine_similarity(emb0, emb2, dim=0).item()\n",
    "    diff_sims.append(sim_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58c096-76b6-4663-8bc0-93d806baa769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# same-image 분포\n",
    "for lv, sims in same_sims.items():\n",
    "    plt.hist(sims, bins=20, alpha=0.5, label=f\"same lv{lv}\")\n",
    "# diff-image 분포\n",
    "plt.hist(diff_sims, bins=20, alpha=0.5, label=\"diff lv0\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Same vs Diff Image Similarity Distributions\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
