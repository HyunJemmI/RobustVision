{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b46406-ee9f-4dce-ac32-334f5988b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ 3626ê°œì˜ 20.jpg íŒŒì¼ì„ '/home1/hyunje0/contrastive_test_set'ì— ë³µì‚¬í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# ì…ë ¥ ë£¨íŠ¸ (TuSimple ì›ë³¸)\n",
    "input_root = \"/home1/hyunje0/.cache/kagglehub/datasets/manideep1108/tusimple/versions/5/TUSimple/train_set/clips\"\n",
    "\n",
    "# ì¶œë ¥ ë£¨íŠ¸ (Test Set ëŒ€ìš©)\n",
    "output_root = \"/home1/hyunje0/contrastive_test_set\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "# ì‚¬ìš© í´ë¦½ ë¦¬ìŠ¤íŠ¸\n",
    "clip_dirs = [\"0313-1\", \"0313-2\", \"0531\", \"0601\"]\n",
    "\n",
    "# counter = 0\n",
    "# for clip in clip_dirs:\n",
    "#     clip_path = os.path.join(input_root, clip)\n",
    "#     for timestamp in os.listdir(clip_path):\n",
    "#         timestamp_path = os.path.join(clip_path, timestamp)\n",
    "#         target_image = os.path.join(timestamp_path, \"20.jpg\")\n",
    "\n",
    "#         if os.path.isfile(target_image):\n",
    "#             # ìƒˆë¡œìš´ íŒŒì¼ëª…: clip_timestamp.jpg\n",
    "#             new_filename = f\"{clip}_{timestamp}.jpg\"\n",
    "#             shutil.copy(target_image, os.path.join(output_root, new_filename))\n",
    "#             counter += 1\n",
    "\n",
    "counter = 0\n",
    "for clip in clip_dirs:\n",
    "    clip_path = os.path.join(input_root, clip)\n",
    "    for timestamp in os.listdir(clip_path):\n",
    "        timestamp_path = os.path.join(clip_path, timestamp)\n",
    "        target_image = os.path.join(timestamp_path, \"20.jpg\")\n",
    "\n",
    "        if os.path.isfile(target_image):\n",
    "            counter += 1\n",
    "        \n",
    "\n",
    "print(f\"âœ… ì´ {counter}ê°œì˜ 20.jpg íŒŒì¼ì„ '{output_root}'ì— ë³µì‚¬í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceea3144-8d0d-4c9c-a010-30fbfe9d0a2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /myubai/lib/python3.12/site-packages (24.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.3.1\n",
      "    Uninstalling pip-24.3.1:\n",
      "      Successfully uninstalled pip-24.3.1\n",
      "Successfully installed pip-25.1.1\n",
      "Requirement already satisfied: tqdm in /myubai/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: pillow in /myubai/lib/python3.12/site-packages (11.1.0)\n",
      "Requirement already satisfied: numpy in /myubai/lib/python3.12/site-packages (2.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install tqdm pillow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa297b8-51b7-44d5-8c29-560c1099ca3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "# â”€â”€â”€ 1. SimCLRModel ì •ì˜ (í•™ìŠµ ëª¨ë¸ê³¼ ë™ì¼í•˜ê²Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class SimCLRModel(nn.Module):\n",
    "    def __init__(self, backbone='resnet18', projection_dim=256):\n",
    "        super().__init__()\n",
    "        base = torchvision.models.resnet50(weights=None) if backbone == 'resnet18' else torchvision.models.resnet18(weights=None)\n",
    "        self.encoder = nn.Sequential(*list(base.children())[:-1])  # remove fc\n",
    "        feat_dim = base.fc.in_features\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)            # shape: [B, feat_dim, 1, 1]\n",
    "        h = h.view(h.size(0), -1)      # flatten: [B, feat_dim]\n",
    "        z = self.projector(h)          # [B, projection_dim]\n",
    "        return F.normalize(z, dim=1)   # L2 normalize across feature dim\n",
    "\n",
    "\n",
    "# â”€â”€â”€ 2. ë…¸ì´ì¦ˆ ì¶”ê°€ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def add_light_noise_with_level(img: Image.Image, level: int) -> Image.Image:\n",
    "    if level == 0:\n",
    "        return img.copy()\n",
    "\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "    radius = 5 * level\n",
    "    intensity = 0.15 * level\n",
    "    blur = img.filter(ImageFilter.GaussianBlur(radius))\n",
    "    arr = np.clip(arr + intensity * np.array(blur), 0, 255)\n",
    "\n",
    "    h, w, _ = arr.shape\n",
    "    cx = int(w * random.uniform(0.3, 0.7))\n",
    "    cy = int(h * random.uniform(0.0, 0.3))\n",
    "    r = int(min(h, w) * 0.1 * level)\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    mask = np.exp(-((x - cx) ** 2 + (y - cy) ** 2) / (2 * (r / 2) ** 2))\n",
    "    arr = np.clip(arr + (mask[..., None] * (0.1 * level)), 0, 255)\n",
    "\n",
    "    gamma = 1.0 + 0.1 * level\n",
    "    inv = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** inv) * 255 for i in range(256)]).astype(np.uint8)\n",
    "    arr = table[arr.astype(np.uint8)]\n",
    "\n",
    "    return Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "\n",
    "# â”€â”€â”€ 3. transform ë° ì„ë² ë”© í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "def get_embedding(model, img: Image.Image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tensor = transform(img).unsqueeze(0).cuda()   # [1, 3, 224, 224]\n",
    "        feat = model(tensor)                          # [1, 256]\n",
    "        return feat.squeeze(0)                        # â†’ [256]\n",
    "\n",
    "\n",
    "# â”€â”€â”€ 4. Cosine Similarity í‰ê°€ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate_similarity(model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model = SimCLRModel(backbone='resnet18', projection_dim=256).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ 5ì¥\n",
    "    test_root = \"/home1/hyunje0/contrastive_test_set\"\n",
    "    image_list = sorted([f for f in os.listdir(test_root) if f.endswith(\".jpg\")])[:5]\n",
    "\n",
    "    # ë¹„êµí•  ë ˆë²¨ ìŒ\n",
    "    compare_pairs = [(0, 1), (0, 3), (0, 5), (0, 8), (3, 7)]\n",
    "\n",
    "    print(\"Cosine Similarity between noise levels using SimCLR model:\\n\")\n",
    "\n",
    "    for fn in tqdm(image_list):\n",
    "        img = Image.open(os.path.join(test_root, fn)).convert(\"RGB\")\n",
    "        level_feats = {}\n",
    "\n",
    "        for lvl in range(9):  # 0~8\n",
    "            noisy = add_light_noise_with_level(img, level=lvl)\n",
    "            feat = get_embedding(model, noisy)\n",
    "            level_feats[lvl] = feat\n",
    "\n",
    "        print(f\"[{fn}]\")\n",
    "        for a, b in compare_pairs:\n",
    "            sim = F.cosine_similarity(level_feats[a], level_feats[b], dim=0).item()\n",
    "            print(f\"  - level {a}-{b}: cosine similarity = {sim:.4f}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# â”€â”€â”€ 5. ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "evaluate_similarity(\"Models/best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebc74982-37e3-47e8-be73-f9cdaeba62e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m     17\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mshow_selected_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mshow_selected_images\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, fname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_list):\n\u001b[0;32m----> 9\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_root, fname))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(img)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_selected_images():\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    test_root = \"/home1/hyunje0/contrastive_test_set\"\n",
    "    image_list = sorted([f for f in os.listdir(test_root) if f.endswith(\".jpg\")])[:5]\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, fname in enumerate(image_list):\n",
    "        img = Image.open(os.path.join(test_root, fname)).convert(\"RGB\")\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(img)\n",
    "    \n",
    "        plt.title(fname, fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(\"Selected TuSimple Test Images (Top 5)\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_selected_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d181d-578a-47ac-a4f3-7c8c202bf40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, random\n",
    "from PIL import Image, ImageFilter\n",
    "def show_augmented_images():\n",
    "    test_root = \"/home1/hyunje0/contrastive_test_set\"\n",
    "    image_list = random.sample(\n",
    "    [f for f in os.listdir(test_root) if f.endswith(\".jpg\")],\n",
    "    k=5)\n",
    "\n",
    "    for fn in image_list:\n",
    "        img = Image.open(os.path.join(test_root, fn)).convert(\"RGB\")\n",
    "        plt.figure(figsize=(18, 2.5))\n",
    "        for lvl in range(9):  # level 0~8\n",
    "            aug = add_light_noise_with_level(img, level=lvl)\n",
    "            plt.subplot(1, 9, lvl+1)\n",
    "            plt.imshow(aug)\n",
    "            plt.title(f\"lv {lvl}\", fontsize=8)\n",
    "            plt.axis(\"off\")\n",
    "        plt.suptitle(f\"{fn} - Noise Levels 0~8\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "show_augmented_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4973f0-0089-4e4b-8bf8-e0b503161f62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€â”€ ë””ë°”ì´ìŠ¤ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "# â”€â”€â”€ ëª¨ë¸ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class SimCLR_Eval(nn.Module):\n",
    "    def __init__(self, proj_dim=128):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet18(weights=None)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-2], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, proj_dim, bias=False), nn.BatchNorm1d(proj_dim, affine=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x).flatten(1)\n",
    "        z = F.normalize(self.projector(h), dim=1)\n",
    "        return z\n",
    "\n",
    "# â”€â”€â”€ ëª¨ë¸ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model = SimCLR_Eval(proj_dim=128).to(device)\n",
    "ckpt = torch.load(\"best_model.pth\", map_location=device)\n",
    "model.load_state_dict(ckpt, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# â”€â”€â”€ ë…¸ì´ì¦ˆ ì¶”ê°€ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def add_light_noise_with_level(img: Image.Image, level: int) -> Image.Image:\n",
    "    if level == 0:\n",
    "        return img.copy()\n",
    "\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "    radius = 5 * level\n",
    "    intensity = 0.15 * level\n",
    "    blur = img.filter(ImageFilter.GaussianBlur(radius))\n",
    "    arr = np.clip(arr + intensity * np.array(blur), 0, 255)\n",
    "\n",
    "    h, w, _ = arr.shape\n",
    "    cx = int(w * random.uniform(0.3, 0.7))\n",
    "    cy = int(h * random.uniform(0.0, 0.3))\n",
    "    r = int(min(h, w) * 0.1 * level)\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    mask = np.exp(-((x - cx) ** 2 + (y - cy) ** 2) / (2 * (r / 2) ** 2))\n",
    "    arr = np.clip(arr + (mask[..., None] * (0.1 * level)), 0, 255)\n",
    "\n",
    "    gamma = 1.0 + 0.1 * level\n",
    "    inv = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** inv) * 255 for i in range(256)]).astype(np.uint8)\n",
    "    arr = table[arr.astype(np.uint8)]\n",
    "\n",
    "    return Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "# â”€â”€â”€ ì „ì²˜ë¦¬ ë° ì„ë² ë”© ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "])\n",
    "\n",
    "def get_embedding(img: Image.Image):\n",
    "    with torch.no_grad():\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        f = model(x).squeeze()\n",
    "        return f / f.norm()\n",
    "\n",
    "# â”€â”€â”€ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_path = \"/home1/hyunje0/contrastive_test_set\"\n",
    "levels = [0, 3, 5, 8]\n",
    "\n",
    "image_list = random.sample(\n",
    "    [f for f in os.listdir(base_path) if f.endswith(\".jpg\")],\n",
    "    k=5\n",
    ")\n",
    "\n",
    "for fname in tqdm(image_list, desc=\"Cosine Similarity on Test Images\"):\n",
    "    img = Image.open(os.path.join(base_path, fname)).convert(\"RGB\")\n",
    "    features = {}\n",
    "    visuals = {}\n",
    "\n",
    "    for lv in levels:\n",
    "        aug = add_light_noise_with_level(img, lv)\n",
    "        feat = get_embedding(aug)\n",
    "        features[lv] = feat\n",
    "        visuals[lv] = aug.copy()\n",
    "\n",
    "    print(f\"\\nğŸ“„ {fname}\")\n",
    "    for lv in levels[1:]:\n",
    "        sim = F.cosine_similarity(features[0], features[lv], dim=0).item()\n",
    "        print(f\"  - lv0 vs lv{lv}: cosine similarity = {sim:.4f}\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "    for i, lv in enumerate(levels):\n",
    "        axes[i].imshow(visuals[lv])\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"lv{lv}\")\n",
    "    plt.suptitle(f\"{fname} - Noise Levels 0/3/5/8\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d2021-bc32-4fb9-bf90-3a54c91fd597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. ëª¨ë¸ ì •ì˜ ë° ë¡œë”©\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class SimCLR_Eval(nn.Module):\n",
    "    def __init__(self, proj_dim=128):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet18(weights=None)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-2], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, proj_dim, bias=False), nn.BatchNorm1d(proj_dim, affine=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x).flatten(1)\n",
    "        z = F.normalize(self.projector(h), dim=1)\n",
    "        return z\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¡œë“œ (encoder + projector ëª¨ë‘ ë¡œë”©)\n",
    "model = SimCLR_Eval(proj_dim=128)\n",
    "ckpt = torch.load(\"Models/best_model.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. ë…¸ì´ì¦ˆ í•¨ìˆ˜ ì •ì˜\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def add_light_noise_with_level(img: Image.Image, level: int) -> Image.Image:\n",
    "    if level == 0:\n",
    "        return img.copy()\n",
    "\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "    radius = 5 * level\n",
    "    intensity = 0.15 * level\n",
    "    blur = img.filter(ImageFilter.GaussianBlur(radius))\n",
    "    arr = np.clip(arr + intensity * np.array(blur), 0, 255)\n",
    "\n",
    "    h, w, _ = arr.shape\n",
    "    cx = int(w * random.uniform(0.3, 0.7))\n",
    "    cy = int(h * random.uniform(0.0, 0.3))\n",
    "    r = int(min(h, w) * 0.1 * level)\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    mask = np.exp(-((x - cx) ** 2 + (y - cy) ** 2) / (2 * (r / 2) ** 2))\n",
    "    arr = np.clip(arr + (mask[..., None] * (0.1 * level)), 0, 255)\n",
    "\n",
    "    gamma = 1.0 + 0.1 * level\n",
    "    inv = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** inv) * 255 for i in range(256)]).astype(np.uint8)\n",
    "    arr = table[arr.astype(np.uint8)]\n",
    "\n",
    "    return Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. ì „ì²˜ë¦¬ ë° ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "])\n",
    "\n",
    "def get_embedding(img: Image.Image):\n",
    "    with torch.no_grad():\n",
    "        x = transform(img).unsqueeze(0)\n",
    "        f = model(x).squeeze()\n",
    "        return f / f.norm()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. ì´ë¯¸ì§€ ë¡œë”© ë° cosine similarity ê³„ì‚°\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_path = \"/home1/hyunje0/contrastive_test_set\"\n",
    "levels = [0, 3, 5, 8]\n",
    "\n",
    "image_list = random.sample(\n",
    "    [f for f in os.listdir(base_path) if f.endswith(\".jpg\")],\n",
    "    k=5\n",
    ")\n",
    "\n",
    "for fname in tqdm(image_list, desc=\"Cosine Similarity on Test Images\"):\n",
    "    img = Image.open(os.path.join(base_path, fname)).convert(\"RGB\")\n",
    "    features = {}\n",
    "    visuals = {}\n",
    "\n",
    "    for lv in levels:\n",
    "        aug = add_light_noise_with_level(img, lv)\n",
    "        feat = get_embedding(aug)\n",
    "        features[lv] = feat\n",
    "        visuals[lv] = aug.copy()\n",
    "\n",
    "    # Cosine Similarity ì¶œë ¥\n",
    "    print(f\"\\nğŸ“„ {fname}\")\n",
    "    for lv in levels[1:]:\n",
    "        sim = F.cosine_similarity(features[0], features[lv], dim=0).item()\n",
    "        print(f\"  - lv0 vs lv{lv}: cosine similarity = {sim:.4f}\")\n",
    "\n",
    "    # ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "    for i, lv in enumerate(levels):\n",
    "        axes[i].imshow(visuals[lv])\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"lv{lv}\")\n",
    "    plt.suptitle(f\"{fname} - Noise Levels 0/3/5/8\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16973ed-5b23-4e5a-98ee-b2a0b3147523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0. Config & Device\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "# data_dir = \"/home1/hyunje0/.cache/kagglehub/datasets/solesensei/solesensei_bdd100k/versions/2/aug_8levels/img\"\n",
    "data_dir = \"/home1/hyunje0/contrastive_test_set\"\n",
    "model_path = \"Models/best_model.pth\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. ëª¨ë¸ ì •ì˜ (encoder + projector)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def resnet18_backbone():\n",
    "    m = torchvision.models.resnet18(weights=None)\n",
    "    return nn.Sequential(*list(m.children())[:-2], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = resnet18_backbone()\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 512, bias=False), nn.BatchNorm1d(512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, proj_dim, bias=False), nn.BatchNorm1d(proj_dim, affine=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x).flatten(1)\n",
    "        z = F.normalize(self.projector(h), dim=1)\n",
    "        return z\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. ëª¨ë¸ ë¡œë”©\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model = SimCLR(proj_dim=128).to(device)\n",
    "ckpt = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(ckpt, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Transform + Embedding í•¨ìˆ˜\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "])\n",
    "\n",
    "def get_embedding(img: Image.Image):\n",
    "    with torch.no_grad():\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "        z = model(x).squeeze()\n",
    "        return z / z.norm()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. Collapse ì—¬ë¶€ ê²€ì¦\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "image_list = [f for f in os.listdir(data_dir) if f.endswith((\".jpg\", \".png\"))]\n",
    "sampled = random.sample(image_list, min(100, len(image_list)))  # ìµœëŒ€ 100ì¥\n",
    "\n",
    "feats = []\n",
    "for fname in sampled:\n",
    "    img = Image.open(os.path.join(data_dir, fname)).convert(\"RGB\")\n",
    "    feats.append(get_embedding(img))\n",
    "\n",
    "all_feats = torch.stack(feats)  # [N, D]\n",
    "mean_std = all_feats.std(dim=0).mean().item()\n",
    "\n",
    "print(f\"\\nğŸ“Š í‰ê·  Feature í‘œì¤€í¸ì°¨ (ì°¨ì›ë³„ stdì˜ í‰ê· ): {mean_std:.6f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. í•´ì„\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if mean_std < 0.01:\n",
    "    print(\"âš ï¸ Feature collapse ì˜ì‹¬: ëŒ€ë¶€ë¶„ ì„ë² ë”©ì´ ë™ì¼í•©ë‹ˆë‹¤.\")\n",
    "elif mean_std < 0.05:\n",
    "    print(\"âš ï¸ Feature ë‹¤ì–‘ì„±ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŒ (ì•½í•œ collapse ê°€ëŠ¥)\")\n",
    "else:\n",
    "    print(\"âœ… Feature ë‹¤ì–‘ì„±ì´ ì¶©ë¶„í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5aed50-06bc-4dbd-a215-1ef66f087a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ (GPU ì‚¬ìš© ì¤‘) â€” GPU ì´ë¦„: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ (CPU ì‚¬ìš© ì¤‘)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7028a-8c65-4b3a-9806-e9340d68019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------êµ¬ë¶„ì„ ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51955842-5aab-477a-a3c1-d9bd0f0bc4a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_imgs \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(base_path) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      2\u001b[0m base_imgs \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(all_imgs, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "all_imgs = [f for f in os.listdir(base_path) if f.endswith(\".jpg\")]\n",
    "base_imgs = random.sample(all_imgs, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7712997-d997-48c9-8b4e-845db6b86c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 1. ë°ì´í„° ì¤€ë¹„\n",
    "all_imgs = [f for f in os.listdir(base_path) if f.endswith(\".jpg\")]\n",
    "base_imgs = random.sample(all_imgs, k=100)\n",
    "\n",
    "same_sims = defaultdict(list)\n",
    "diff_sims = []\n",
    "\n",
    "for fname in tqdm(base_imgs, desc=\"Experiment loops\"):\n",
    "    img = Image.open(os.path.join(base_path, fname)).convert(\"RGB\")\n",
    "    emb0 = get_embedding(img)\n",
    "\n",
    "    # 2. ë™ì¼ ì´ë¯¸ì§€ vs ë…¸ì´ì¦ˆ\n",
    "    for lv in [3,5,8]:\n",
    "        aug = add_light_noise_with_level(img, lv)\n",
    "        embn = get_embedding(aug)\n",
    "        sim = F.cosine_similarity(emb0, embn, dim=0).item()\n",
    "        same_sims[lv].append(sim)\n",
    "\n",
    "    # 3. ì„œë¡œ ë‹¤ë¥¸ ì´ë¯¸ì§€ ê°„ ìœ ì‚¬ë„\n",
    "    other = random.choice([x for x in all_imgs if x != fname])\n",
    "    img2 = Image.open(os.path.join(base_path, other)).convert(\"RGB\")\n",
    "    emb2 = get_embedding(img2)\n",
    "    sim_diff = F.cosine_similarity(emb0, emb2, dim=0).item()\n",
    "    diff_sims.append(sim_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58c096-76b6-4663-8bc0-93d806baa769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# same-image ë¶„í¬\n",
    "for lv, sims in same_sims.items():\n",
    "    plt.hist(sims, bins=20, alpha=0.5, label=f\"same lv{lv}\")\n",
    "# diff-image ë¶„í¬\n",
    "plt.hist(diff_sims, bins=20, alpha=0.5, label=\"diff lv0\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Same vs Diff Image Similarity Distributions\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
